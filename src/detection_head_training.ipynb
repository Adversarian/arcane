{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Idea\n",
    "Generate 1000 attacks and use the following features to fit a detection head on them for decision making:\n",
    "1. $MSE(x, f_{recon}(x|\\hat{c}))$\n",
    "2. $D(x)$ (veracity)\n",
    "3. $p(\\hat{c}|x)$ from $D_{aux}$\n",
    "5. $p(\\hat{c}|x)$ from the victim\n",
    "6. $JS(victim(x), D_{aux}(x))$\n",
    "4. $\\log(D(x)) + \\log(p(\\hat{c}|x))$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# REMEMBER: Convert all outputs to probability distributions!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys  # TODO: Fix this?\n",
    "\n",
    "sys.path.append(\"PyTorch-StudioGAN/\")\n",
    "import itertools\n",
    "import random\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms.functional as TF\n",
    "from Attacks.attacks_suite import *\n",
    "from Classifiers.resnet import load_model as load_victim_model\n",
    "from Defense.def_utils import *\n",
    "import json\n",
    "from functools import partial\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from gan_loader import load_model as load_gan_model\n",
    "from LGDPM.diffusion_loader import load_model as load_diffusion_model\n",
    "from sklearn.model_selection import GridSearchCV, train_test_split\n",
    "from torchmetrics.functional.classification import binary_auroc, multiclass_accuracy\n",
    "from torchmetrics.functional.regression import kl_divergence\n",
    "from torchvision import datasets, transforms as T\n",
    "from tqdm import tqdm\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "assert torch.cuda.is_available()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 42\n",
    "torch.backends.cudnn.enabled = True\n",
    "torch.backends.cudnn.deterministic = True\n",
    "\n",
    "random.seed(seed);\n",
    "np.random.seed(seed);\n",
    "torch.cuda.manual_seed(seed);\n",
    "torch.manual_seed(seed);\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NormalizeInverse(T.Normalize):\n",
    "    def __init__(self, mean, std, *args, **kwargs):\n",
    "        mean = torch.as_tensor(mean)\n",
    "        std = torch.as_tensor(std)\n",
    "        std_inv = 1 / (std + 1e-7)\n",
    "        mean_inv = -mean * std_inv\n",
    "        super().__init__(mean=mean_inv, std=std_inv, *args, **kwargs)\n",
    "\n",
    "    def __call__(self, tensor):\n",
    "        return super().__call__(tensor.clone())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def JSD(p, q, log_prob=False):\n",
    "    m = 0.5 * (p + q)\n",
    "    return 0.5 * kl_divergence(p, m, log_prob=log_prob) + 0.5 * kl_divergence(\n",
    "        q, m, log_prob=log_prob\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "V_WEIGHTS = {\n",
    "    \"MNIST\": \"saved_models/victims/mnist_resnet18.pth\",\n",
    "    \"CIFAR10\": \"saved_models/victims/cifar10_resnet18.pth\",\n",
    "    \"TIMGNET\": \"saved_models/victims/timgnet_resnet18.pth\",\n",
    "}\n",
    "cifar10_victim = load_victim_model(V_WEIGHTS[\"CIFAR10\"], num_classes=10, in_channels=3)\n",
    "timgnet_victim = load_victim_model(V_WEIGHTS[\"TIMGNET\"], num_classes=200, in_channels=3)\n",
    "cifar10_victim.eval()\n",
    "timgnet_victim.eval()\n",
    "VICTIM_MODELS = {\n",
    "    \"CIFAR10\": cifar10_victim,\n",
    "    \"TIMGNET\": timgnet_victim,\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "D_WEIGHTS = {\n",
    "    \"CIFAR10_DIFF\": \"saved_models/defenders/diffusion/cifar10_diff.pth\",\n",
    "    \"TIMGNET_DIFF\": \"saved_models/defenders/diffusion/timgnet_diff.pth\",\n",
    "    \"CIFAR10_REACGAN\": (\n",
    "        \"saved_models/defenders/acgan/cifar10/G_cifar10_reacgan.pth\",\n",
    "        \"saved_models/defenders/acgan/cifar10/D_cifar10_reacgan.pth\",\n",
    "    ),\n",
    "    \"TIMGNET_REACGAN\": (\n",
    "        \"saved_models/defenders/acgan/timgnet/G_timgnet_reacgan.pth\",\n",
    "        \"saved_models/defenders/acgan/timgnet/D_timgnet_reacgan.pth\",\n",
    "    ),\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_sample_baskets(n_samples=200):\n",
    "    all_datasets = {\n",
    "        \"CIFAR10\": datasets.CIFAR10(\n",
    "            root=\"PyTorch-StudioGAN/data/\",\n",
    "            transform=T.Compose(\n",
    "                [\n",
    "                    T.ToTensor(),\n",
    "                    T.Normalize(\n",
    "                        DEFAULT_MOMENTS.CIFAR10.mean, DEFAULT_MOMENTS.CIFAR10.std\n",
    "                    ),\n",
    "                ]\n",
    "            ),\n",
    "        ),\n",
    "        \"TIMGNET\": datasets.ImageFolder(\n",
    "            root=\"PyTorch-StudioGAN/data/tiny-imagenet-200/train/\",\n",
    "            transform=T.Compose(\n",
    "                [\n",
    "                    T.ToTensor(),\n",
    "                    T.Normalize(\n",
    "                        DEFAULT_MOMENTS.TIMGNET.mean, DEFAULT_MOMENTS.TIMGNET.std\n",
    "                    ),\n",
    "                ]\n",
    "            ),\n",
    "        ),\n",
    "    }\n",
    "    samples = {}\n",
    "    for name, ds in all_datasets.items():\n",
    "        targets = torch.tensor(ds.targets)\n",
    "        all_classes = torch.unique(targets)\n",
    "        assert (\n",
    "            n_samples % len(all_classes) == 0\n",
    "        ), f\"Cannot sample equally with the provided n_samples={n_samples} and n_targets={len(all_classes)}\"\n",
    "        n_samples_per_class = n_samples // len(all_classes)\n",
    "        balanced_sample_indices = []\n",
    "        for cls in all_classes:\n",
    "            only_cls = torch.where(targets == cls)[0].tolist()\n",
    "            balanced_sample_indices.extend(\n",
    "                random.sample(only_cls, n_samples_per_class)\n",
    "            )\n",
    "        samples.update({name: [ds[idx] for idx in balanced_sample_indices]})\n",
    "    return samples\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_attacks(victim, ds_name, real_sample_baskets, targeted=False, **kwargs):\n",
    "    images, labels = zip(*real_sample_baskets[ds_name])\n",
    "    images, labels = torch.stack(images).to(\"cuda\"), torch.tensor(list(labels)).to(\n",
    "        \"cuda\", dtype=torch.long\n",
    "    )\n",
    "    if isinstance(targeted, bool):\n",
    "        suite = partial(run_attack_suite, targets=\"auto\" if targeted else None)\n",
    "    elif isinstance(targeted, torch.Tensor):\n",
    "        suite = partial(run_attack_suite, targets=targeted)\n",
    "    attacks = suite(victim, ds_name, images, labels, **kwargs)\n",
    "    torch.cuda.empty_cache()\n",
    "    return attacks\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "if os.path.exists(\"1k_real_sample_baskets.pth\"):\n",
    "    real_sample_baskets = torch.load(\"1k_real_sample_baskets.pth\")\n",
    "else:\n",
    "    real_sample_baskets = generate_sample_baskets(n_samples=1000)\n",
    "    torch.save(real_sample_baskets, \"1k_real_sample_baskets.pth\")\n",
    "if os.path.exists(\"1k_ADV_ATTACKS.pth\"):\n",
    "    ADV_ATTACKS = torch.load(\"1k_ADV_ATTACKS.pth\")\n",
    "else:\n",
    "    ADV_ATTACKS = {\n",
    "        \"CIFAR10\": generate_attacks(\n",
    "            cifar10_victim, \"CIFAR10\", real_sample_baskets, targeted=True, splits=10\n",
    "        ),\n",
    "        \"TIMGNET\": generate_attacks(\n",
    "            timgnet_victim, \"TIMGNET\", real_sample_baskets, targeted=True, splits=10\n",
    "        ),\n",
    "    }\n",
    "    torch.save(ADV_ATTACKS, \"1k_ADV_ATTACKS.pth\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: have attacks and clean images here\n",
    "# -> Discriminator Outputs\n",
    "# -> Victim outputs\n",
    "# -> Targeted Reconstruction\n",
    "# -> Make training data\n",
    "# -> Fit XGB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "DEFENDER_DEFAULT_CONFIGS = {\n",
    "    \"DIFF\": {\n",
    "        \"CIFAR10\": \"CIFAR10_DIFF\",\n",
    "        \"TIMGNET\": \"TIMGNET_DIFF\",\n",
    "        \"aux_d_type_for_diff\": \"REACGAN\",\n",
    "    },\n",
    "    \"ACGAN\":{\n",
    "        \"CIFAR10\": \"CIFAR10_REACGAN\",\n",
    "        \"TIMGNET\": \"TIMGNET_REACGAN\",\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "BASE_PATH = \"DHT_models/\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DIFFUSION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.inference_mode()\n",
    "def diffusion_metrics(\n",
    "    conditional_diffuser,\n",
    "    auxiliary_discriminator,\n",
    "    inputs,\n",
    "    labels,\n",
    "    splits=1,\n",
    "    **targeted_purify_kwargs\n",
    "):\n",
    "    num_channels = inputs.shape[1]\n",
    "    discriminator_normalizer = T.Normalize(\n",
    "        mean=DISCRIMINATOR_MOMENTS[num_channels][\"mean\"],\n",
    "        std=DISCRIMINATOR_MOMENTS[num_channels][\"std\"],\n",
    "    )\n",
    "    recon_loss = diffusion_purify_targeted(\n",
    "        conditional_diffuser,\n",
    "        inputs,\n",
    "        labels,\n",
    "        return_purified=False,\n",
    "        splits=splits,\n",
    "        **targeted_purify_kwargs\n",
    "    )\n",
    "    dis_inputs = discriminator_normalizer(inputs).detach()\n",
    "    dis_inputs_chunks = torch.chunk(dis_inputs, splits)\n",
    "    veracity, class_posteriors = [], []\n",
    "    for dis_inputs_chunk in dis_inputs_chunks:\n",
    "        d_result = auxiliary_discriminator.forward_emb(dis_inputs_chunk)\n",
    "        veracity.append(F.sigmoid(d_result[\"adv_output\"].detach()))\n",
    "        class_posteriors.append(F.softmax(d_result[\"cls_output\"].detach(), dim=1))\n",
    "    return recon_loss, torch.cat(veracity), torch.cat(class_posteriors)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "10it [02:34, 15.46s/it]\n",
      "10it [02:24, 14.41s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 20 candidates, totalling 100 fits\n",
      "Best CV score: 0.9970000000000001 - Best params: {'max_depth': 1, 'n_estimators': 50}\n",
      "CIFAR10 - CW XGB clf Score: 0.9995\n",
      "[0.00583138 0.00497438 0.8556871  0.09806246 0.00697095 0.02847368]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "10it [02:22, 14.27s/it]\n",
      "10it [02:22, 14.28s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 20 candidates, totalling 100 fits\n",
      "Best CV score: 0.9345000000000001 - Best params: {'max_depth': 1, 'n_estimators': 50}\n",
      "CIFAR10 - FGSM XGB clf Score: 0.9445\n",
      "[0.01572933 0.01057607 0.11516175 0.7250773  0.01718033 0.11627524]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "10it [02:22, 14.26s/it]\n",
      "10it [02:22, 14.27s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 20 candidates, totalling 100 fits\n",
      "Best CV score: 0.9935 - Best params: {'max_depth': 1, 'n_estimators': 50}\n",
      "CIFAR10 - PGD XGB clf Score: 0.998\n",
      "[0.         0.0088792  0.5938653  0.09941047 0.02393176 0.2739133 ]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "10it [14:12, 85.28s/it]\n",
      "10it [14:22, 86.21s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 20 candidates, totalling 100 fits\n",
      "Best CV score: 0.9515 - Best params: {'max_depth': 3, 'n_estimators': 50}\n",
      "TIMGNET - CW XGB clf Score: 0.973\n",
      "[0.02677916 0.02733272 0.5959568  0.2210356  0.08465185 0.04424397]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "10it [15:15, 91.54s/it]\n",
      "10it [14:49, 88.92s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 20 candidates, totalling 100 fits\n",
      "Best CV score: 0.9844999999999999 - Best params: {'max_depth': 1, 'n_estimators': 50}\n",
      "TIMGNET - FGSM XGB clf Score: 0.9875\n",
      "[0.00554014 0.00526516 0.10966279 0.68457156 0.1291508  0.06580959]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "10it [14:33, 87.39s/it]\n",
      "10it [14:27, 86.73s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 20 candidates, totalling 100 fits\n",
      "Best CV score: 0.9630000000000001 - Best params: {'max_depth': 2, 'n_estimators': 50}\n",
      "TIMGNET - PGD XGB clf Score: 0.9755\n",
      "[0.0131713  0.01470757 0.14458884 0.13044733 0.617773   0.07931195]\n"
     ]
    }
   ],
   "source": [
    "NUM_SPLITS = 10\n",
    "for dataset_name in [\"CIFAR10\", \"TIMGNET\"]:\n",
    "    for attack_method in [\"CW\", \"FGSM\", \"PGD\"]:\n",
    "        training_file_name = os.path.join(\n",
    "            BASE_PATH, \"DIFF\", f\"DIFF_{dataset_name}_{attack_method}_train.pth\"\n",
    "        )\n",
    "        if not os.path.exists(training_file_name):\n",
    "            victim = VICTIM_MODELS[dataset_name]\n",
    "            defender = DEFENDER_DEFAULT_CONFIGS[\"DIFF\"][dataset_name]\n",
    "            aux_d_name = (\n",
    "                dataset_name\n",
    "                + \"_\"\n",
    "                + DEFENDER_DEFAULT_CONFIGS[\"DIFF\"][\"aux_d_type_for_diff\"]\n",
    "            )\n",
    "            _, defender_diff_ema = load_diffusion_model(D_WEIGHTS[defender], defender)\n",
    "            _, defender_D, _ = load_gan_model(\n",
    "                D_WEIGHTS[aux_d_name][0], D_WEIGHTS[aux_d_name][1], aux_d_name\n",
    "            )\n",
    "            defender_D.eval()\n",
    "            defender_diff_ema.eval()\n",
    "            clean_images, clean_true_labels = zip(*real_sample_baskets[dataset_name])\n",
    "            clean_images, clean_true_labels = torch.stack(clean_images).to(\n",
    "                \"cuda\"\n",
    "            ), torch.tensor(list(clean_true_labels)).to(\"cuda\")\n",
    "            adv_denorm_images, adv_norm_images, adv_true_labels = (\n",
    "                ADV_ATTACKS[dataset_name][attack_method][\n",
    "                    \"unnormalized_clipped_samples\"\n",
    "                ],\n",
    "                ADV_ATTACKS[dataset_name][attack_method][\"normalized_clipped_samples\"],\n",
    "                ADV_ATTACKS[dataset_name][\"clean_labels\"],\n",
    "            )\n",
    "            inverse_normalizer = NormalizeInverse(\n",
    "                mean=DEFAULT_MOMENTS[dataset_name][\"mean\"],\n",
    "                std=DEFAULT_MOMENTS[dataset_name][\"std\"],\n",
    "            )\n",
    "            ### CLEAN\n",
    "            # clean images are normalized!\n",
    "            clean_images_chunks = torch.chunk(clean_images, NUM_SPLITS)\n",
    "            clean_victim_class_posteriors = F.softmax(\n",
    "                torch.cat(\n",
    "                    [victim(chunk).detach() for chunk in clean_images_chunks], dim=0\n",
    "                ),\n",
    "                dim=1,\n",
    "            )\n",
    "            clean_victim_labels = torch.argmax(clean_victim_class_posteriors, dim=1)\n",
    "            clean_victim_max_class_posteriors = clean_victim_class_posteriors.gather(\n",
    "                dim=1, index=clean_victim_labels[:, None]\n",
    "            ).squeeze()\n",
    "            (\n",
    "                clean_recon_loss,\n",
    "                clean_veracity,\n",
    "                clean_dis_class_posteriors,\n",
    "            ) = diffusion_metrics(\n",
    "                defender_diff_ema,\n",
    "                defender_D,\n",
    "                inverse_normalizer(clean_images),\n",
    "                clean_victim_labels,\n",
    "                splits=NUM_SPLITS,\n",
    "                disable_tqdm=False,\n",
    "            )\n",
    "            clean_dis_max_class_posteriors = clean_dis_class_posteriors.gather(\n",
    "                dim=1, index=clean_victim_labels[:, None]\n",
    "            ).squeeze()\n",
    "            clean_jsd = torch.as_tensor(\n",
    "                list(\n",
    "                    map(\n",
    "                        JSD,\n",
    "                        clean_victim_class_posteriors.unsqueeze(-2),\n",
    "                        clean_dis_class_posteriors.unsqueeze(-2),\n",
    "                    )\n",
    "                )\n",
    "            )\n",
    "            clean_sum_of_logs = torch.log(clean_veracity) + torch.log(\n",
    "                clean_dis_max_class_posteriors\n",
    "            )\n",
    "\n",
    "            ### ADVERSARIAL\n",
    "            adv_images_chunks = torch.chunk(adv_norm_images, NUM_SPLITS)\n",
    "            adv_victim_class_posteriors = F.softmax(\n",
    "                torch.cat(\n",
    "                    [victim(chunk).detach() for chunk in adv_images_chunks],\n",
    "                    dim=0,\n",
    "                ),\n",
    "                dim=1,\n",
    "            )\n",
    "            adv_victim_labels = torch.argmax(adv_victim_class_posteriors, dim=1)\n",
    "            adv_victim_max_class_posteriors = adv_victim_class_posteriors.gather(\n",
    "                dim=1, index=adv_victim_labels[:, None]\n",
    "            ).squeeze()\n",
    "            adv_recon_loss, adv_veracity, adv_dis_class_posteriors = diffusion_metrics(\n",
    "                defender_diff_ema,\n",
    "                defender_D,\n",
    "                adv_denorm_images,\n",
    "                adv_victim_labels,\n",
    "                splits=NUM_SPLITS,\n",
    "                disable_tqdm=False,\n",
    "            )\n",
    "            adv_dis_max_class_posteriors = adv_dis_class_posteriors.gather(\n",
    "                dim=1, index=adv_victim_labels[:, None]\n",
    "            ).squeeze()\n",
    "            adv_jsd = torch.as_tensor(\n",
    "                list(\n",
    "                    map(\n",
    "                        JSD,\n",
    "                        adv_victim_class_posteriors.unsqueeze(-2),\n",
    "                        adv_dis_class_posteriors.unsqueeze(-2),\n",
    "                    )\n",
    "                )\n",
    "            )\n",
    "            adv_sum_of_logs = torch.log(adv_veracity) + torch.log(\n",
    "                adv_dis_max_class_posteriors\n",
    "            )\n",
    "\n",
    "            ### AGGREGATE\n",
    "            clean_X_train = torch.vstack(\n",
    "                [\n",
    "                    clean_recon_loss.cpu(),\n",
    "                    clean_veracity.cpu(),\n",
    "                    clean_dis_max_class_posteriors.cpu(),\n",
    "                    clean_victim_max_class_posteriors.cpu(),\n",
    "                    clean_jsd.cpu(),\n",
    "                    clean_sum_of_logs.cpu(),\n",
    "                ]\n",
    "            )\n",
    "            adv_X_train = torch.vstack(\n",
    "                [\n",
    "                    adv_recon_loss.cpu(),\n",
    "                    adv_veracity.cpu(),\n",
    "                    adv_dis_max_class_posteriors.cpu(),\n",
    "                    adv_victim_max_class_posteriors.cpu(),\n",
    "                    adv_jsd.cpu(),\n",
    "                    adv_sum_of_logs.cpu(),\n",
    "                ]\n",
    "            )\n",
    "            X_train = torch.vstack([clean_X_train.T, adv_X_train.T]).numpy()\n",
    "            y_train = torch.vstack(\n",
    "                [\n",
    "                    torch.zeros(clean_X_train.shape[1], 1),\n",
    "                    torch.ones(adv_X_train.shape[1], 1),\n",
    "                ]\n",
    "            ).numpy()\n",
    "            training_data = {\n",
    "                \"X\": X_train,\n",
    "                \"y\": y_train,\n",
    "            }\n",
    "            torch.save(training_data, training_file_name)\n",
    "        else:\n",
    "            training_data = torch.load(training_file_name)\n",
    "            X_train, y_train = training_data[\"X\"], training_data[\"y\"]\n",
    "\n",
    "        ### TRAIN\n",
    "        clf_file_name = os.path.join(\n",
    "            BASE_PATH, \"DIFF\", f\"DIFF_{dataset_name}_{attack_method}_XGB.json\"\n",
    "        )\n",
    "        gridsearch_hist_file_name = os.path.join(\n",
    "            BASE_PATH, \"DIFF\", f\"DIFF_{dataset_name}_{attack_method}_GridSearchCV.hist\"\n",
    "        )\n",
    "        best_params_file_name = os.path.join(\n",
    "            BASE_PATH, \"DIFF\", f\"DIFF_{dataset_name}_{attack_method}_best_params.json\"\n",
    "        )\n",
    "        base_xgb_model = XGBClassifier(objective=\"binary:logistic\", eval_metric=\"auc\")\n",
    "        gridsearch_clf = GridSearchCV(\n",
    "            base_xgb_model,\n",
    "            {\n",
    "                \"max_depth\": [1, 2, 3, 4, 5],\n",
    "                \"n_estimators\": [2, 5, 10, 50],\n",
    "            },\n",
    "            verbose=1,\n",
    "        )\n",
    "        gridsearch_clf.fit(X_train, y_train)\n",
    "        print(\n",
    "            f\"Best CV score: {gridsearch_clf.best_score_} - Best params: {gridsearch_clf.best_params_}\"\n",
    "        )\n",
    "        torch.save(gridsearch_clf.cv_results_, gridsearch_hist_file_name)\n",
    "        with open(best_params_file_name, \"w\") as hist_fp:\n",
    "            json.dump(gridsearch_clf.best_params_, hist_fp)\n",
    "        best_xgb_clf = XGBClassifier(\n",
    "            objective=\"binary:logistic\",\n",
    "            eval_metric=\"auc\",\n",
    "            **gridsearch_clf.best_params_,\n",
    "        )\n",
    "        best_xgb_clf.fit(X_train, y_train)\n",
    "        print(\n",
    "            f\"{dataset_name} - {attack_method} XGB clf Score: {best_xgb_clf.score(X_train, y_train)}\"\n",
    "        )\n",
    "        best_xgb_clf.save_model(clf_file_name)\n",
    "        print(best_xgb_clf.feature_importances_)\n",
    "        torch.cuda.empty_cache()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ACGAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def acgan_metrics(\n",
    "    conditional_generator,\n",
    "    auxiliary_discriminator,\n",
    "    inputs,\n",
    "    labels,\n",
    "    z_dim,\n",
    "    splits=1,\n",
    "    **targeted_purify_kwargs\n",
    "):\n",
    "    num_channels = inputs.shape[1]\n",
    "    discriminator_normalizer = T.Normalize(\n",
    "        mean=DISCRIMINATOR_MOMENTS[num_channels][\"mean\"],\n",
    "        std=DISCRIMINATOR_MOMENTS[num_channels][\"std\"],\n",
    "    )\n",
    "    recon_loss = acgan_purify_targeted(\n",
    "        conditional_generator,\n",
    "        inputs,\n",
    "        labels,\n",
    "        z_dim,\n",
    "        return_purified=False,\n",
    "        splits=splits,\n",
    "        **targeted_purify_kwargs\n",
    "    )\n",
    "    dis_inputs = discriminator_normalizer(inputs).detach()\n",
    "    dis_inputs_chunks = torch.chunk(dis_inputs, splits)\n",
    "    veracity, class_posteriors = [], []\n",
    "    for dis_inputs_chunk in dis_inputs_chunks:\n",
    "        d_result = auxiliary_discriminator.forward_emb(dis_inputs_chunk)\n",
    "        veracity.append(F.sigmoid(d_result[\"adv_output\"].detach()))\n",
    "        class_posteriors.append(F.softmax(d_result[\"cls_output\"].detach(), dim=1))\n",
    "    return recon_loss, torch.cat(veracity), torch.cat(class_posteriors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ACGAN Purify Targeted: Optimizing Z: 100%|██████████| 500/500 [00:43<00:00, 11.40it/s]\n",
      "ACGAN Purify Targeted: Optimizing Z: 100%|██████████| 500/500 [00:44<00:00, 11.31it/s]\n",
      "ACGAN Purify Targeted: Optimizing Z: 100%|██████████| 500/500 [00:44<00:00, 11.32it/s]\n",
      "ACGAN Purify Targeted: Optimizing Z: 100%|██████████| 500/500 [00:44<00:00, 11.33it/s]\n",
      "ACGAN Purify Targeted: Optimizing Z: 100%|██████████| 500/500 [00:44<00:00, 11.31it/s]\n",
      "ACGAN Purify Targeted: Optimizing Z: 100%|██████████| 500/500 [00:44<00:00, 11.31it/s]\n",
      "ACGAN Purify Targeted: Optimizing Z: 100%|██████████| 500/500 [00:44<00:00, 11.30it/s]\n",
      "ACGAN Purify Targeted: Optimizing Z: 100%|██████████| 500/500 [00:44<00:00, 11.31it/s]\n",
      "ACGAN Purify Targeted: Optimizing Z: 100%|██████████| 500/500 [00:44<00:00, 11.34it/s]\n",
      "ACGAN Purify Targeted: Optimizing Z: 100%|██████████| 500/500 [00:44<00:00, 11.31it/s]\n",
      "10it [07:21, 44.19s/it]\n",
      "ACGAN Purify Targeted: Optimizing Z: 100%|██████████| 500/500 [00:43<00:00, 11.40it/s]\n",
      "ACGAN Purify Targeted: Optimizing Z: 100%|██████████| 500/500 [00:44<00:00, 11.31it/s]\n",
      "ACGAN Purify Targeted: Optimizing Z: 100%|██████████| 500/500 [00:44<00:00, 11.32it/s]\n",
      "ACGAN Purify Targeted: Optimizing Z: 100%|██████████| 500/500 [00:44<00:00, 11.31it/s]\n",
      "ACGAN Purify Targeted: Optimizing Z: 100%|██████████| 500/500 [00:44<00:00, 11.32it/s]\n",
      "ACGAN Purify Targeted: Optimizing Z: 100%|██████████| 500/500 [00:44<00:00, 11.29it/s]\n",
      "ACGAN Purify Targeted: Optimizing Z: 100%|██████████| 500/500 [00:44<00:00, 11.29it/s]\n",
      "ACGAN Purify Targeted: Optimizing Z: 100%|██████████| 500/500 [00:44<00:00, 11.33it/s]\n",
      "ACGAN Purify Targeted: Optimizing Z: 100%|██████████| 500/500 [00:44<00:00, 11.30it/s]\n",
      "ACGAN Purify Targeted: Optimizing Z: 100%|██████████| 500/500 [00:44<00:00, 11.33it/s]\n",
      "10it [07:22, 44.21s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 20 candidates, totalling 100 fits\n",
      "Best CV score: 0.9970000000000001 - Best params: {'max_depth': 1, 'n_estimators': 50}\n",
      "CIFAR10 - CW XGB clf Score: 0.9995\n",
      "[0.         0.00584522 0.86298263 0.09538148 0.00707443 0.02871611]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ACGAN Purify Targeted: Optimizing Z: 100%|██████████| 500/500 [00:43<00:00, 11.41it/s]\n",
      "ACGAN Purify Targeted: Optimizing Z: 100%|██████████| 500/500 [00:44<00:00, 11.28it/s]\n",
      "ACGAN Purify Targeted: Optimizing Z: 100%|██████████| 500/500 [00:44<00:00, 11.29it/s]\n",
      "ACGAN Purify Targeted: Optimizing Z: 100%|██████████| 500/500 [00:44<00:00, 11.29it/s]\n",
      "ACGAN Purify Targeted: Optimizing Z: 100%|██████████| 500/500 [00:44<00:00, 11.28it/s]\n",
      "ACGAN Purify Targeted: Optimizing Z: 100%|██████████| 500/500 [00:44<00:00, 11.28it/s]\n",
      "ACGAN Purify Targeted: Optimizing Z: 100%|██████████| 500/500 [00:44<00:00, 11.27it/s]\n",
      "ACGAN Purify Targeted: Optimizing Z: 100%|██████████| 500/500 [00:44<00:00, 11.27it/s]\n",
      "ACGAN Purify Targeted: Optimizing Z: 100%|██████████| 500/500 [00:44<00:00, 11.30it/s]\n",
      "ACGAN Purify Targeted: Optimizing Z: 100%|██████████| 500/500 [00:44<00:00, 11.28it/s]\n",
      "10it [07:23, 44.31s/it]\n",
      "ACGAN Purify Targeted: Optimizing Z: 100%|██████████| 500/500 [00:43<00:00, 11.37it/s]\n",
      "ACGAN Purify Targeted: Optimizing Z: 100%|██████████| 500/500 [00:44<00:00, 11.27it/s]\n",
      "ACGAN Purify Targeted: Optimizing Z: 100%|██████████| 500/500 [00:44<00:00, 11.27it/s]\n",
      "ACGAN Purify Targeted: Optimizing Z: 100%|██████████| 500/500 [00:44<00:00, 11.27it/s]\n",
      "ACGAN Purify Targeted: Optimizing Z: 100%|██████████| 500/500 [00:44<00:00, 11.28it/s]\n",
      "ACGAN Purify Targeted: Optimizing Z: 100%|██████████| 500/500 [00:44<00:00, 11.28it/s]\n",
      "ACGAN Purify Targeted: Optimizing Z: 100%|██████████| 500/500 [00:44<00:00, 11.27it/s]\n",
      "ACGAN Purify Targeted: Optimizing Z: 100%|██████████| 500/500 [00:44<00:00, 11.27it/s]\n",
      "ACGAN Purify Targeted: Optimizing Z: 100%|██████████| 500/500 [00:44<00:00, 11.29it/s]\n",
      "ACGAN Purify Targeted: Optimizing Z: 100%|██████████| 500/500 [00:44<00:00, 11.28it/s]\n",
      "10it [07:23, 44.35s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 20 candidates, totalling 100 fits\n",
      "Best CV score: 0.9404999999999999 - Best params: {'max_depth': 1, 'n_estimators': 50}\n",
      "CIFAR10 - FGSM XGB clf Score: 0.948\n",
      "[0.01429752 0.01099937 0.12091529 0.71401864 0.0177051  0.12206416]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ACGAN Purify Targeted: Optimizing Z: 100%|██████████| 500/500 [00:43<00:00, 11.41it/s]\n",
      "ACGAN Purify Targeted: Optimizing Z: 100%|██████████| 500/500 [00:44<00:00, 11.28it/s]\n",
      "ACGAN Purify Targeted: Optimizing Z: 100%|██████████| 500/500 [00:44<00:00, 11.28it/s]\n",
      "ACGAN Purify Targeted: Optimizing Z: 100%|██████████| 500/500 [00:44<00:00, 11.29it/s]\n",
      "ACGAN Purify Targeted: Optimizing Z: 100%|██████████| 500/500 [00:44<00:00, 11.28it/s]\n",
      "ACGAN Purify Targeted: Optimizing Z: 100%|██████████| 500/500 [00:44<00:00, 11.27it/s]\n",
      "ACGAN Purify Targeted: Optimizing Z: 100%|██████████| 500/500 [00:44<00:00, 11.27it/s]\n",
      "ACGAN Purify Targeted: Optimizing Z: 100%|██████████| 500/500 [00:44<00:00, 11.27it/s]\n",
      "ACGAN Purify Targeted: Optimizing Z: 100%|██████████| 500/500 [00:44<00:00, 11.30it/s]\n",
      "ACGAN Purify Targeted: Optimizing Z: 100%|██████████| 500/500 [00:44<00:00, 11.27it/s]\n",
      "10it [07:23, 44.32s/it]\n",
      "ACGAN Purify Targeted: Optimizing Z: 100%|██████████| 500/500 [00:43<00:00, 11.39it/s]\n",
      "ACGAN Purify Targeted: Optimizing Z: 100%|██████████| 500/500 [00:44<00:00, 11.29it/s]\n",
      "ACGAN Purify Targeted: Optimizing Z: 100%|██████████| 500/500 [00:44<00:00, 11.31it/s]\n",
      "ACGAN Purify Targeted: Optimizing Z: 100%|██████████| 500/500 [00:44<00:00, 11.29it/s]\n",
      "ACGAN Purify Targeted: Optimizing Z: 100%|██████████| 500/500 [00:44<00:00, 11.31it/s]\n",
      "ACGAN Purify Targeted: Optimizing Z: 100%|██████████| 500/500 [00:44<00:00, 11.30it/s]\n",
      "ACGAN Purify Targeted: Optimizing Z: 100%|██████████| 500/500 [00:44<00:00, 11.29it/s]\n",
      "ACGAN Purify Targeted: Optimizing Z: 100%|██████████| 500/500 [00:44<00:00, 11.33it/s]\n",
      "ACGAN Purify Targeted: Optimizing Z: 100%|██████████| 500/500 [00:44<00:00, 11.30it/s]\n",
      "ACGAN Purify Targeted: Optimizing Z: 100%|██████████| 500/500 [00:44<00:00, 11.33it/s]\n",
      "10it [07:22, 44.23s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 20 candidates, totalling 100 fits\n",
      "Best CV score: 0.9935 - Best params: {'max_depth': 1, 'n_estimators': 50}\n",
      "CIFAR10 - PGD XGB clf Score: 0.998\n",
      "[0.         0.0088792  0.5938653  0.09941047 0.02393176 0.2739133 ]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ACGAN Purify Targeted: Optimizing Z: 100%|██████████| 500/500 [01:05<00:00,  7.69it/s]\n",
      "ACGAN Purify Targeted: Optimizing Z: 100%|██████████| 500/500 [01:05<00:00,  7.63it/s]\n",
      "ACGAN Purify Targeted: Optimizing Z: 100%|██████████| 500/500 [01:05<00:00,  7.63it/s]\n",
      "ACGAN Purify Targeted: Optimizing Z: 100%|██████████| 500/500 [01:05<00:00,  7.63it/s]\n",
      "ACGAN Purify Targeted: Optimizing Z: 100%|██████████| 500/500 [01:05<00:00,  7.63it/s]\n",
      "ACGAN Purify Targeted: Optimizing Z: 100%|██████████| 500/500 [01:05<00:00,  7.63it/s]\n",
      "ACGAN Purify Targeted: Optimizing Z: 100%|██████████| 500/500 [01:05<00:00,  7.63it/s]\n",
      "ACGAN Purify Targeted: Optimizing Z: 100%|██████████| 500/500 [01:05<00:00,  7.63it/s]\n",
      "ACGAN Purify Targeted: Optimizing Z: 100%|██████████| 500/500 [01:05<00:00,  7.60it/s]\n",
      "ACGAN Purify Targeted: Optimizing Z: 100%|██████████| 500/500 [01:09<00:00,  7.23it/s]\n",
      "10it [10:59, 65.93s/it]\n",
      "ACGAN Purify Targeted: Optimizing Z: 100%|██████████| 500/500 [01:04<00:00,  7.77it/s]\n",
      "ACGAN Purify Targeted: Optimizing Z: 100%|██████████| 500/500 [01:04<00:00,  7.72it/s]\n",
      "ACGAN Purify Targeted: Optimizing Z: 100%|██████████| 500/500 [01:04<00:00,  7.72it/s]\n",
      "ACGAN Purify Targeted: Optimizing Z: 100%|██████████| 500/500 [01:04<00:00,  7.71it/s]\n",
      "ACGAN Purify Targeted: Optimizing Z: 100%|██████████| 500/500 [01:04<00:00,  7.72it/s]\n",
      "ACGAN Purify Targeted: Optimizing Z: 100%|██████████| 500/500 [01:04<00:00,  7.73it/s]\n",
      "ACGAN Purify Targeted: Optimizing Z: 100%|██████████| 500/500 [01:04<00:00,  7.73it/s]\n",
      "ACGAN Purify Targeted: Optimizing Z: 100%|██████████| 500/500 [01:04<00:00,  7.72it/s]\n",
      "ACGAN Purify Targeted: Optimizing Z: 100%|██████████| 500/500 [01:04<00:00,  7.72it/s]\n",
      "ACGAN Purify Targeted: Optimizing Z: 100%|██████████| 500/500 [01:04<00:00,  7.72it/s]\n",
      "10it [10:47, 64.78s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 20 candidates, totalling 100 fits\n",
      "Best CV score: 0.9515 - Best params: {'max_depth': 3, 'n_estimators': 50}\n",
      "TIMGNET - CW XGB clf Score: 0.9725\n",
      "[0.01939472 0.02565976 0.61750895 0.22071359 0.0800981  0.03662479]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ACGAN Purify Targeted: Optimizing Z: 100%|██████████| 500/500 [01:04<00:00,  7.79it/s]\n",
      "ACGAN Purify Targeted: Optimizing Z: 100%|██████████| 500/500 [01:04<00:00,  7.72it/s]\n",
      "ACGAN Purify Targeted: Optimizing Z: 100%|██████████| 500/500 [01:04<00:00,  7.71it/s]\n",
      "ACGAN Purify Targeted: Optimizing Z: 100%|██████████| 500/500 [01:04<00:00,  7.72it/s]\n",
      "ACGAN Purify Targeted: Optimizing Z: 100%|██████████| 500/500 [01:05<00:00,  7.68it/s]\n",
      "ACGAN Purify Targeted: Optimizing Z: 100%|██████████| 500/500 [01:04<00:00,  7.70it/s]\n",
      "ACGAN Purify Targeted: Optimizing Z: 100%|██████████| 500/500 [01:05<00:00,  7.63it/s]\n",
      "ACGAN Purify Targeted: Optimizing Z: 100%|██████████| 500/500 [01:05<00:00,  7.67it/s]\n",
      "ACGAN Purify Targeted: Optimizing Z: 100%|██████████| 500/500 [01:05<00:00,  7.63it/s]\n",
      "ACGAN Purify Targeted: Optimizing Z: 100%|██████████| 500/500 [01:05<00:00,  7.61it/s]\n",
      "10it [10:50, 65.10s/it]\n",
      "ACGAN Purify Targeted: Optimizing Z: 100%|██████████| 500/500 [01:05<00:00,  7.66it/s]\n",
      "ACGAN Purify Targeted: Optimizing Z: 100%|██████████| 500/500 [01:05<00:00,  7.64it/s]\n",
      "ACGAN Purify Targeted: Optimizing Z: 100%|██████████| 500/500 [01:05<00:00,  7.64it/s]\n",
      "ACGAN Purify Targeted: Optimizing Z: 100%|██████████| 500/500 [01:05<00:00,  7.63it/s]\n",
      "ACGAN Purify Targeted: Optimizing Z: 100%|██████████| 500/500 [01:05<00:00,  7.60it/s]\n",
      "ACGAN Purify Targeted: Optimizing Z: 100%|██████████| 500/500 [01:05<00:00,  7.60it/s]\n",
      "ACGAN Purify Targeted: Optimizing Z: 100%|██████████| 500/500 [01:05<00:00,  7.61it/s]\n",
      "ACGAN Purify Targeted: Optimizing Z: 100%|██████████| 500/500 [01:05<00:00,  7.60it/s]\n",
      "ACGAN Purify Targeted: Optimizing Z: 100%|██████████| 500/500 [01:05<00:00,  7.61it/s]\n",
      "ACGAN Purify Targeted: Optimizing Z: 100%|██████████| 500/500 [01:05<00:00,  7.61it/s]\n",
      "10it [10:56, 65.68s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 20 candidates, totalling 100 fits\n",
      "Best CV score: 0.984 - Best params: {'max_depth': 1, 'n_estimators': 50}\n",
      "TIMGNET - FGSM XGB clf Score: 0.987\n",
      "[0.00441428 0.00552707 0.11697416 0.6908551  0.12165723 0.06057219]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ACGAN Purify Targeted: Optimizing Z: 100%|██████████| 500/500 [01:04<00:00,  7.70it/s]\n",
      "ACGAN Purify Targeted: Optimizing Z: 100%|██████████| 500/500 [01:05<00:00,  7.63it/s]\n",
      "ACGAN Purify Targeted: Optimizing Z: 100%|██████████| 500/500 [01:05<00:00,  7.64it/s]\n",
      "ACGAN Purify Targeted: Optimizing Z: 100%|██████████| 500/500 [01:05<00:00,  7.63it/s]\n",
      "ACGAN Purify Targeted: Optimizing Z: 100%|██████████| 500/500 [01:05<00:00,  7.64it/s]\n",
      "ACGAN Purify Targeted: Optimizing Z: 100%|██████████| 500/500 [01:05<00:00,  7.63it/s]\n",
      "ACGAN Purify Targeted: Optimizing Z: 100%|██████████| 500/500 [01:05<00:00,  7.63it/s]\n",
      "ACGAN Purify Targeted: Optimizing Z: 100%|██████████| 500/500 [01:05<00:00,  7.63it/s]\n",
      "ACGAN Purify Targeted: Optimizing Z: 100%|██████████| 500/500 [01:05<00:00,  7.63it/s]\n",
      "ACGAN Purify Targeted: Optimizing Z: 100%|██████████| 500/500 [01:05<00:00,  7.64it/s]\n",
      "10it [10:54, 65.49s/it]\n",
      "ACGAN Purify Targeted: Optimizing Z: 100%|██████████| 500/500 [01:05<00:00,  7.66it/s]\n",
      "ACGAN Purify Targeted: Optimizing Z: 100%|██████████| 500/500 [01:05<00:00,  7.64it/s]\n",
      "ACGAN Purify Targeted: Optimizing Z: 100%|██████████| 500/500 [01:05<00:00,  7.64it/s]\n",
      "ACGAN Purify Targeted: Optimizing Z: 100%|██████████| 500/500 [01:05<00:00,  7.65it/s]\n",
      "ACGAN Purify Targeted: Optimizing Z: 100%|██████████| 500/500 [01:05<00:00,  7.65it/s]\n",
      "ACGAN Purify Targeted: Optimizing Z: 100%|██████████| 500/500 [01:05<00:00,  7.64it/s]\n",
      "ACGAN Purify Targeted: Optimizing Z: 100%|██████████| 500/500 [01:05<00:00,  7.64it/s]\n",
      "ACGAN Purify Targeted: Optimizing Z: 100%|██████████| 500/500 [01:05<00:00,  7.64it/s]\n",
      "ACGAN Purify Targeted: Optimizing Z: 100%|██████████| 500/500 [01:05<00:00,  7.64it/s]\n",
      "ACGAN Purify Targeted: Optimizing Z: 100%|██████████| 500/500 [01:05<00:00,  7.64it/s]\n",
      "10it [10:54, 65.48s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 20 candidates, totalling 100 fits\n",
      "Best CV score: 0.959 - Best params: {'max_depth': 3, 'n_estimators': 50}\n",
      "TIMGNET - PGD XGB clf Score: 0.9875\n",
      "[0.01349808 0.01824572 0.12875135 0.07903984 0.6781514  0.08231358]\n"
     ]
    }
   ],
   "source": [
    "NUM_SPLITS = 10\n",
    "for dataset_name in [\"CIFAR10\", \"TIMGNET\"]:\n",
    "    for attack_method in [\"CW\", \"FGSM\", \"PGD\"]:\n",
    "        training_file_name = os.path.join(\n",
    "            BASE_PATH, \"ACGAN\", f\"ACGAN_{dataset_name}_{attack_method}_train.pth\"\n",
    "        )\n",
    "        if not os.path.exists(training_file_name):\n",
    "            victim = VICTIM_MODELS[dataset_name]\n",
    "            defender = DEFENDER_DEFAULT_CONFIGS[\"ACGAN\"][dataset_name]\n",
    "            defender_G, defender_D, z_dim = load_gan_model(\n",
    "                D_WEIGHTS[defender][0], D_WEIGHTS[defender][1], defender\n",
    "            )\n",
    "            defender_G.eval()\n",
    "            defender_D.eval()\n",
    "            clean_images, clean_true_labels = zip(*real_sample_baskets[dataset_name])\n",
    "            clean_images, clean_true_labels = torch.stack(clean_images).to(\n",
    "                \"cuda\"\n",
    "            ), torch.tensor(list(clean_true_labels)).to(\"cuda\")\n",
    "            adv_denorm_images, adv_norm_images, adv_true_labels = (\n",
    "                ADV_ATTACKS[dataset_name][attack_method][\n",
    "                    \"unnormalized_clipped_samples\"\n",
    "                ],\n",
    "                ADV_ATTACKS[dataset_name][attack_method][\"normalized_clipped_samples\"],\n",
    "                ADV_ATTACKS[dataset_name][\"clean_labels\"],\n",
    "            )\n",
    "            inverse_normalizer = NormalizeInverse(\n",
    "                mean=DEFAULT_MOMENTS[dataset_name][\"mean\"],\n",
    "                std=DEFAULT_MOMENTS[dataset_name][\"std\"],\n",
    "            )\n",
    "            ### CLEAN\n",
    "            clean_images_chunks = torch.chunk(clean_images, NUM_SPLITS)\n",
    "            clean_victim_class_posteriors = F.softmax(\n",
    "                torch.cat(\n",
    "                    [victim(chunk).detach() for chunk in clean_images_chunks], dim=0\n",
    "                ),\n",
    "                dim=1,\n",
    "            )\n",
    "            clean_victim_labels = torch.argmax(clean_victim_class_posteriors, dim=1)\n",
    "            clean_victim_max_class_posteriors = clean_victim_class_posteriors.gather(\n",
    "                dim=1, index=clean_victim_labels[:, None]\n",
    "            ).squeeze()\n",
    "            (\n",
    "                clean_recon_loss,\n",
    "                clean_veracity,\n",
    "                clean_dis_class_posteriors,\n",
    "            ) = acgan_metrics(\n",
    "                defender_G,\n",
    "                defender_D,\n",
    "                inverse_normalizer(clean_images),\n",
    "                clean_victim_labels,\n",
    "                z_dim,\n",
    "                splits=NUM_SPLITS,\n",
    "                disable_tqdm=False,\n",
    "            )\n",
    "            clean_dis_max_class_posteriors = clean_dis_class_posteriors.gather(\n",
    "                dim=1, index=clean_victim_labels[:, None]\n",
    "            ).squeeze()\n",
    "            clean_jsd = torch.as_tensor(\n",
    "                list(\n",
    "                    map(\n",
    "                        JSD,\n",
    "                        clean_victim_class_posteriors.unsqueeze(-2),\n",
    "                        clean_dis_class_posteriors.unsqueeze(-2),\n",
    "                    )\n",
    "                )\n",
    "            )\n",
    "            clean_sum_of_logs = torch.log(clean_veracity) + torch.log(\n",
    "                clean_dis_max_class_posteriors\n",
    "            )\n",
    "\n",
    "            ### ADVERSARIAL\n",
    "            adv_images_chunks = torch.chunk(adv_norm_images, NUM_SPLITS)\n",
    "            adv_victim_class_posteriors = F.softmax(\n",
    "                torch.cat(\n",
    "                    [victim(chunk).detach() for chunk in adv_images_chunks],\n",
    "                    dim=0,\n",
    "                ),\n",
    "                dim=1,\n",
    "            )\n",
    "            adv_victim_labels = torch.argmax(adv_victim_class_posteriors, dim=1)\n",
    "            adv_victim_max_class_posteriors = adv_victim_class_posteriors.gather(\n",
    "                dim=1, index=adv_victim_labels[:, None]\n",
    "            ).squeeze()\n",
    "            adv_recon_loss, adv_veracity, adv_dis_class_posteriors = acgan_metrics(\n",
    "                defender_G,\n",
    "                defender_D,\n",
    "                adv_denorm_images,\n",
    "                adv_victim_labels,\n",
    "                z_dim,\n",
    "                splits=NUM_SPLITS,\n",
    "                disable_tqdm=False,\n",
    "            )\n",
    "            adv_dis_max_class_posteriors = adv_dis_class_posteriors.gather(\n",
    "                dim=1, index=adv_victim_labels[:, None]\n",
    "            ).squeeze()\n",
    "            adv_jsd = torch.as_tensor(\n",
    "                list(\n",
    "                    map(\n",
    "                        JSD,\n",
    "                        adv_victim_class_posteriors.unsqueeze(-2),\n",
    "                        adv_dis_class_posteriors.unsqueeze(-2),\n",
    "                    )\n",
    "                )\n",
    "            )\n",
    "            adv_sum_of_logs = torch.log(adv_veracity) + torch.log(\n",
    "                adv_dis_max_class_posteriors\n",
    "            )\n",
    "\n",
    "            ### AGGREGATE\n",
    "            clean_X_train = torch.vstack(\n",
    "                [\n",
    "                    clean_recon_loss.detach().cpu(),\n",
    "                    clean_veracity.detach().cpu(),\n",
    "                    clean_dis_max_class_posteriors.detach().cpu(),\n",
    "                    clean_victim_max_class_posteriors.detach().cpu(),\n",
    "                    clean_jsd.detach().cpu(),\n",
    "                    clean_sum_of_logs.detach().cpu(),\n",
    "                ]\n",
    "            )\n",
    "            adv_X_train = torch.vstack(\n",
    "                [\n",
    "                    adv_recon_loss.detach().cpu(),\n",
    "                    adv_veracity.detach().cpu(),\n",
    "                    adv_dis_max_class_posteriors.detach().cpu(),\n",
    "                    adv_victim_max_class_posteriors.detach().cpu(),\n",
    "                    adv_jsd.detach().cpu(),\n",
    "                    adv_sum_of_logs.detach().cpu(),\n",
    "                ]\n",
    "            )\n",
    "            X_train = torch.vstack([clean_X_train.T, adv_X_train.T]).numpy()\n",
    "            y_train = torch.vstack(\n",
    "                [\n",
    "                    torch.zeros(clean_X_train.shape[1], 1),\n",
    "                    torch.ones(adv_X_train.shape[1], 1),\n",
    "                ]\n",
    "            ).numpy()\n",
    "            training_data = {\n",
    "                \"X\": X_train,\n",
    "                \"y\": y_train,\n",
    "            }\n",
    "            torch.save(training_data, training_file_name)\n",
    "        else:\n",
    "            training_data = torch.load(training_file_name)\n",
    "            X_train, y_train = training_data[\"X\"], training_data[\"y\"]\n",
    "\n",
    "        ### TRAIN\n",
    "        clf_file_name = os.path.join(\n",
    "            BASE_PATH, \"ACGAN\", f\"ACGAN_{dataset_name}_{attack_method}_XGB.json\"\n",
    "        )\n",
    "        gridsearch_hist_file_name = os.path.join(\n",
    "            BASE_PATH,\n",
    "            \"ACGAN\",\n",
    "            f\"ACGAN_{dataset_name}_{attack_method}_GridSearchCV.hist\",\n",
    "        )\n",
    "        best_params_file_name = os.path.join(\n",
    "            BASE_PATH, \"ACGAN\", f\"ACGAN_{dataset_name}_{attack_method}_best_params.json\"\n",
    "        )\n",
    "        base_xgb_model = XGBClassifier(objective=\"binary:logistic\", eval_metric=\"auc\")\n",
    "        gridsearch_clf = GridSearchCV(\n",
    "            base_xgb_model,\n",
    "            {\n",
    "                \"max_depth\": [1, 2, 3, 4, 5],\n",
    "                \"n_estimators\": [2, 5, 10, 50],\n",
    "            },\n",
    "            verbose=1,\n",
    "        )\n",
    "        gridsearch_clf.fit(X_train, y_train)\n",
    "        print(\n",
    "            f\"Best CV score: {gridsearch_clf.best_score_} - Best params: {gridsearch_clf.best_params_}\"\n",
    "        )\n",
    "        torch.save(gridsearch_clf.cv_results_, gridsearch_hist_file_name)\n",
    "        with open(best_params_file_name, \"w\") as hist_fp:\n",
    "            json.dump(gridsearch_clf.best_params_, hist_fp)\n",
    "        best_xgb_clf = XGBClassifier(\n",
    "            objective=\"binary:logistic\",\n",
    "            eval_metric=\"auc\",\n",
    "            **gridsearch_clf.best_params_,\n",
    "        )\n",
    "        best_xgb_clf.fit(X_train, y_train)\n",
    "        print(\n",
    "            f\"{dataset_name} - {attack_method} XGB clf Score: {best_xgb_clf.score(X_train, y_train)}\"\n",
    "        )\n",
    "        best_xgb_clf.save_model(clf_file_name)\n",
    "        print(best_xgb_clf.feature_importances_)\n",
    "        torch.cuda.empty_cache()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
